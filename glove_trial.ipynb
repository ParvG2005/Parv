{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIDFDFsXgQrruNgYDlf9Xx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParvG2005/Parv/blob/main/glove_trial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Za3MBYzGog6",
        "outputId": "10aab2bc-bdb5-4be6-8540-602dbf9d6619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "started training\n",
            "started training\n",
            "started training\n",
            "Iteration 1, Loss=0.0386\n",
            "Iteration 2, Loss=0.0291\n",
            "Iteration 3, Loss=0.0225\n",
            "Iteration 4, Loss=0.0177\n",
            "Iteration 5, Loss=0.0147\n",
            "Iteration 6, Loss=0.0130\n",
            "Iteration 7, Loss=0.0120\n",
            "Iteration 8, Loss=0.0113\n",
            "Iteration 9, Loss=0.0107\n",
            "Iteration 10, Loss=0.0103\n",
            "started training\n",
            "Validation Perplexity: 6948.649944028533\n",
            "Test Perplexity: 5756.309410452827\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Load & preprocess dataset\n",
        "# -----------------------------\n",
        "def load_tokens(filename, min_count=3):\n",
        "    with open(filename, encoding=\"utf-8\") as f:\n",
        "        text = f.read().lower()\n",
        "    tokens = re.findall(r\"\\b\\w+\\b\", text)\n",
        "\n",
        "    # Replace rare words with <UNK>\n",
        "    freq = Counter(tokens)\n",
        "    tokens = [t if freq[t] >= min_count else \"<UNK>\" for t in tokens]\n",
        "    return tokens\n",
        "\n",
        "train_tokens = load_tokens(\"wiki_movie_plots_deduped.csv\")\n",
        "valid_tokens = load_tokens(\"english_15000.txt\")\n",
        "test_tokens  = load_tokens(\"english_test.txt\")\n",
        "\n",
        "# Vocabulary\n",
        "vocab = list(set(train_tokens))\n",
        "word2id = {w:i for i,w in enumerate(vocab)}\n",
        "id2word = {i:w for w,i in word2id.items()}\n",
        "V = len(vocab)\n",
        "print(\"started training\")\n",
        "# -----------------------------\n",
        "# 2. Build co-occurrence matrix\n",
        "# -----------------------------\n",
        "def build_cooccurrence(tokens, window_size=5):\n",
        "    cooccurrence = defaultdict(float)\n",
        "    for idx, word in enumerate(tokens):\n",
        "        if word not in word2id:   # safety\n",
        "            continue\n",
        "        w_id = word2id[word]\n",
        "        start = max(0, idx - window_size)\n",
        "        end   = min(len(tokens), idx + window_size + 1)\n",
        "        for j in range(start, end):\n",
        "            if j != idx and tokens[j] in word2id:\n",
        "                c_id = word2id[tokens[j]]\n",
        "                cooccurrence[(w_id, c_id)] += 1.0 / abs(j - idx)\n",
        "    return cooccurrence\n",
        "\n",
        "cooc = build_cooccurrence(train_tokens)\n",
        "print(\"started training\")\n",
        "# -----------------------------\n",
        "# 3. GloVe model training\n",
        "# -----------------------------\n",
        "def glove_train_optimized(cooc, vector_size=50, iterations=50, x_max=100, alpha=0.75, lr=0.05, batch_size=2048):\n",
        "    i_ids, j_ids, Xij = zip(*[(i,j,x) for (i,j),x in cooc.items()])\n",
        "    i_ids, j_ids, Xij = np.array(i_ids), np.array(j_ids), np.array(Xij, dtype=np.float32)\n",
        "    logX = np.log(Xij)\n",
        "\n",
        "    # Initialize parameters\n",
        "    W = np.random.randn(V, vector_size) / np.sqrt(vector_size)\n",
        "    W_tilde = np.random.randn(V, vector_size) / np.sqrt(vector_size)\n",
        "    b = np.zeros(V)\n",
        "    b_tilde = np.zeros(V)\n",
        "\n",
        "    # AdaGrad accumulators\n",
        "    grad_sq_W = np.ones_like(W)\n",
        "    grad_sq_Wt = np.ones_like(W_tilde)\n",
        "    grad_sq_b = np.ones_like(b)\n",
        "    grad_sq_bt = np.ones_like(b_tilde)\n",
        "\n",
        "    n_pairs = len(Xij)\n",
        "\n",
        "    for it in range(iterations):\n",
        "        perm = np.random.permutation(n_pairs)\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for start in range(0, n_pairs, batch_size):\n",
        "            end = min(start+batch_size, n_pairs)\n",
        "            idx = perm[start:end]\n",
        "\n",
        "            i, j, x, lx = i_ids[idx], j_ids[idx], Xij[idx], logX[idx]\n",
        "\n",
        "            # f(x)\n",
        "            f = np.where(x < x_max, (x/x_max)**alpha, 1.0)\n",
        "\n",
        "            # Predictions\n",
        "            dot = np.sum(W[i] * W_tilde[j], axis=1) + b[i] + b_tilde[j]\n",
        "            cost = dot - lx\n",
        "            total_loss += np.sum(0.5 * f * cost**2)\n",
        "\n",
        "            # Gradients\n",
        "            grad_common = f * cost\n",
        "            grad_Wi = grad_common[:, None] * W_tilde[j]\n",
        "            grad_Wj = grad_common[:, None] * W[i]\n",
        "            grad_bi = grad_common\n",
        "            grad_bj = grad_common\n",
        "\n",
        "            # AdaGrad updates\n",
        "            W[i]       -= (lr / np.sqrt(grad_sq_W[i])) * grad_Wi\n",
        "            W_tilde[j] -= (lr / np.sqrt(grad_sq_Wt[j])) * grad_Wj\n",
        "            b[i]       -= (lr / np.sqrt(grad_sq_b[i])) * grad_bi\n",
        "            b_tilde[j] -= (lr / np.sqrt(grad_sq_bt[j])) * grad_bj\n",
        "\n",
        "            # Accumulate squared gradients\n",
        "            grad_sq_W[i]       += grad_Wi**2\n",
        "            grad_sq_Wt[j]      += grad_Wj**2\n",
        "            grad_sq_b[i]       += grad_bi**2\n",
        "            grad_sq_bt[j]      += grad_bj**2\n",
        "\n",
        "        print(f\"Iteration {it+1}, Loss={total_loss/len(Xij):.4f}\")\n",
        "\n",
        "    return W + W_tilde\n",
        "print(\"started training\")\n",
        "# Train model\n",
        "embeddings = glove_train_optimized(cooc, vector_size=50, iterations=10)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Perplexity evaluation\n",
        "# -----------------------------\n",
        "def perplexity(tokens, embeddings, word2id, window_size=2):\n",
        "    N = len(tokens)\n",
        "    log_prob = 0\n",
        "    count = 0\n",
        "    for i in range(window_size, N):\n",
        "        context_ids = [word2id.get(tokens[j], word2id[\"<UNK>\"]) for j in range(i-window_size, i)]\n",
        "        target_id = word2id.get(tokens[i], word2id[\"<UNK>\"])\n",
        "\n",
        "        context_vec = np.mean([embeddings[c] for c in context_ids], axis=0)\n",
        "\n",
        "        # Stable softmax\n",
        "        scores = embeddings @ context_vec\n",
        "        scores -= np.max(scores)\n",
        "        exp_scores = np.exp(scores)\n",
        "        probs = exp_scores / np.sum(exp_scores)\n",
        "\n",
        "        log_prob += np.log(probs[target_id] + 1e-10)\n",
        "        count += 1\n",
        "\n",
        "    return np.exp(-log_prob / count)\n",
        "print(\"started training\")\n",
        "val_perp = perplexity(valid_tokens, embeddings, word2id)\n",
        "test_perp = perplexity(test_tokens, embeddings, word2id)\n",
        "\n",
        "print(\"Validation Perplexity:\", val_perp)\n",
        "print(\"Test Perplexity:\", test_perp)\n"
      ]
    }
  ]
}